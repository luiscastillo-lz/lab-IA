# üß™ PLAN DE ACCI√ìN - RAG LABIA
## Sistema de Asistencia Virtual para Laboratorio de Control de Calidad

**Fecha:** 27 de diciembre de 2025  
**Proyecto:** LabIa - Asistente Virtual para T√©cnicos de Laboratorio  
**Tecnolog√≠as Core:** LangChain + Google Gemini 2.5 Flash + PostgreSQL + pgvector

---

## üéØ OBJETIVO

Crear un RAG especializado que permita a t√©cnicos de control de calidad consultar instructivos y procedimientos de ensayos de laboratorio mediante un chatbot inteligente que:

1. Procesa PDFs t√©cnicos con tablas, figuras, normas ASTM y variables especializadas
2. Mantiene contexto conversacional (√∫ltimas 5 interacciones)
3. Responde con Chain of Thought y restricci√≥n estricta al contexto
4. Normaliza unidades t√©cnicas (¬∞C, psi, MPa, mm, in)
5. Extrae metadatos normativos (c√≥digos LL-CI-I-xx, ASTM, revisiones)

---

## üîß DECISIONES T√âCNICAS TOMADAS

### **1. Chunking Strategy** ‚úÖ Opci√≥n B
- **Tama√±o:** 1024 tokens por chunk
- **Overlap:** 150 tokens
- **Justificaci√≥n:** Balance √≥ptimo entre contexto y granularidad para procedimientos t√©cnicos complejos
- **Implementaci√≥n:** RecursiveCharacterTextSplitter con separadores sem√°nticos

### **2. OCR Strategy** ‚úÖ Opci√≥n A
- **Approach:** Solo pytesseract para im√°genes/diagramas cuando extractores nativos fallen
- **Prioridad:**
  1. pdfplumber (tablas estructuradas)
  2. PyMuPDF/fitz (texto nativo)
  3. pytesseract (OCR como fallback para im√°genes)
- **Justificaci√≥n:** Optimiza velocidad de ingesta, reduce costo computacional
- **Validaci√≥n:** Evaluar calidad con 2-3 PDFs representativos primero

### **3. Normalizaci√≥n de Unidades** ‚úÖ Opci√≥n B
- **Approach:** Guardar AMBAS versiones en metadatos
  - `valor_original`: "25 psi", "100 ¬∞C"
  - `valor_normalizado`: "172.37 kPa", "373.15 K"
- **Librer√≠a:** `pint` para conversiones SI
- **Justificaci√≥n:** M√°xima flexibilidad en queries y respuestas t√©cnicas precisas
- **Implementaci√≥n:** Diccionario de t√©rminos con regex para detecci√≥n

### **4. Flujo de Desarrollo** ‚úÖ Local ‚Üí Docker
1. **Fase Local:**
   - Crear venv Python 3.11.9
   - Instalar dependencias
   - Probar ingesta 2-3 PDFs
   - Validar queries y contexto
   - Debugging r√°pido
2. **Fase Docker:**
   - Migrar a contenedores
   - Probar ingesta completa (44 PDFs)
   - Validar producci√≥n

---

## üìã PLAN DE IMPLEMENTACI√ìN PASO A PASO

### **FASE 1: CONFIGURACI√ìN INICIAL** üèóÔ∏è

#### **Paso 1.1: Analizar Estructura Actual**
- [x] Leer app.py, database.py, ingest.py existentes
- [x] Identificar tecnolog√≠as actuales (OpenAI ‚Üí Google Gemini)
- [x] Revisar static/* (HTML/CSS/JS)
- [x] Examinar docker-compose.yml y Dockerfile
- [x] Listar PDFs en raw/ (44 instructivos LL-CI-I/LL-CII)

#### **Paso 1.2: Crear requirements.txt**
```txt
# Framework Web
flask==3.0.0
python-dotenv==1.0.0

# LangChain + Google Gemini
langchain==0.1.0
langchain-google-genai==0.0.5
langchain-community==0.0.13
google-generativeai==0.3.2

# Vector Store
langchain-postgres==0.0.2
psycopg2-binary==2.9.9
pgvector==0.2.4

# Procesamiento PDFs - Multi-Librer√≠a
pdfplumber==0.10.3        # Tablas estructuradas
pypdf==3.17.4             # Parser b√°sico
PyMuPDF==1.23.8           # fitz - Texto e im√°genes
tabula-py==2.9.0          # Tablas complejas (requiere Java)

# OCR
pytesseract==0.3.10       # OCR para im√°genes
Pillow==10.1.0            # Procesamiento im√°genes
pdf2image==1.16.3         # Convertir PDF a imagen

# Procesamiento Avanzado
unstructured==0.11.6      # Segmentaci√≥n sem√°ntica
pandas==2.1.4             # Manejo de datos tabulares

# Normalizaci√≥n
pint==0.23                # Conversi√≥n de unidades
regex==2023.12.25         # Regex avanzado

# Utilidades
tiktoken==0.5.2           # Conteo de tokens
httpx==0.25.2             # HTTP client
certifi==2023.11.17       # SSL certificates
```

---

### **FASE 2: DESARROLLO BACKEND** ‚öôÔ∏è

#### **Paso 2.1: Crear ingest.py - Pipeline de Ingesta Avanzada**

**Componentes:**

1. **Extractor Multi-Librer√≠a**
```python
class PDFExtractor:
    def extract_text(self, pdf_path):
        # 1. PyMuPDF para texto nativo
        # 2. pdfplumber para tablas
        # 3. pytesseract para im√°genes (fallback)
```

2. **Limpieza y Normalizaci√≥n**
```python
def clean_headers_footers(text):
    # Regex para eliminar "DOCUMENTO CONTROLADO", logos, pies repetidos
    
def normalize_units(text):
    # Detectar y convertir: ¬∞C, psi, MPa, mm, in, kg, etc.
    # Retornar: {original: "25 psi", normalized: "172.37 kPa"}
```

3. **Extracci√≥n de Metadatos**
```python
def extract_metadata(pdf_path, text):
    return {
        'codigo_documento': extract_code(text),  # LL-CI-I-05, LL-CII-20
        'norma_astm': extract_astm(text),        # ASTM C109, C1090
        'revision': extract_revision(text),       # rev01, rev02
        'fecha': extract_date(text),
        'variables_tecnicas': extract_variables(text),  # pH, Pa, Ps, G, T, V
    }
```

4. **Segmentaci√≥n Sem√°ntica**
```python
def segment_by_sections(text):
    sections = {
        'INICIO': ...,
        'OBJETIVO': ...,
        'ALCANCE': ...,
        'REQUISITOS': ...,
        'PROCEDIMIENTO': ...,
        'TABLA': ...,
        'FIGURA': ...,
        'PRECAUCIONES': ...,
        'FIN': ...
    }
    # Usar unstructured para etiquetar tipo de bloque
```

5. **Chunking Controlado**
```python
RecursiveCharacterTextSplitter(
    chunk_size=1024,        # Tokens
    chunk_overlap=150,      # Tokens
    separators=["\n\n", "\n", ". ", " ", ""],
    keep_separator=True
)
# Evitar mezclar tabla + narrativa en el mismo chunk
```

6. **Embeddings + PostgreSQL**
```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

vectorstore = PGVector(
    collection_name="labia_embeddings",
    embedding_function=embeddings,
    connection_string="postgresql://..."
)
```

**Flujo Completo:**
```
raw/*.pdf ‚Üí PDFExtractor ‚Üí Clean ‚Üí Segment ‚Üí Extract Metadata ‚Üí 
Normalize Units ‚Üí Chunk (1024/150) ‚Üí Embeddings ‚Üí PostgreSQL+pgvector
```

---

#### **Paso 2.2: Modificar database.py**

**Cambios principales:**

1. **Embeddings: OpenAI ‚Üí Google Gemini**
```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY")
)
```

2. **Colecci√≥n: lazarito_embeddings ‚Üí labia_embeddings**

3. **Nuevas Tablas PostgreSQL:**
```sql
-- Historial conversacional (√∫ltimas 5 interacciones)
CREATE TABLE chat_session_state (
    session_id VARCHAR PRIMARY KEY,
    user_id VARCHAR,
    conversation_history JSONB,  -- [{"role": "user", "content": "..."}, ...]
    last_interaction TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Logs de conversaci√≥n
CREATE TABLE chat_logs (
    id SERIAL PRIMARY KEY,
    session_id VARCHAR,
    query TEXT,
    response TEXT,
    context_docs JSONB,
    tokens_input INTEGER,
    tokens_output INTEGER,
    cost_usd DECIMAL(10,6),
    latency_ms INTEGER,
    timestamp TIMESTAMP DEFAULT NOW()
);

-- Feedback negativo
CREATE TABLE negative_feedbacks (
    id SERIAL PRIMARY KEY,
    session_id VARCHAR,
    query TEXT,
    response TEXT,
    feedback_text TEXT,
    timestamp TIMESTAMP DEFAULT NOW()
);
```

4. **Funciones para Historial Conversacional:**
```python
def save_conversation_turn(session_id, role, content):
    # Guardar turno en chat_session_state
    # Mantener solo √∫ltimas 5 interacciones
    
def get_conversation_history(session_id, limit=5):
    # Recuperar √∫ltimas 5 interacciones
    return [
        {"role": "user", "content": "¬øC√≥mo medir pH?"},
        {"role": "assistant", "content": "Seg√∫n LLCCI02..."}
    ]
```

---

#### **Paso 2.3: Crear app.py - API Flask con Google Gemini**

**Configuraci√≥n Expl√≠cita LLM:**
```python
from langchain_google_genai import ChatGoogleGenerativeAI

# CONFIGURACI√ìN EXPL√çCITA DEL LLM
LLM_CONFIG = {
    "model": "gemini-2.5-flash-latest",  # Modelo configurable
    "temperature": 0.3,                   # Configurable (0-1)
    "max_output_tokens": 2048,           # Configurable
    "top_p": 0.95,
    "top_k": 40
}

llm = ChatGoogleGenerativeAI(
    model=LLM_CONFIG["model"],
    temperature=LLM_CONFIG["temperature"],
    max_output_tokens=LLM_CONFIG["max_output_tokens"],
    google_api_key=os.getenv("GOOGLE_API_KEY")
)
```

**System Prompt Especializado:**
```python
SYSTEM_PROMPT = """Eres un asistente virtual del laboratorio de control de calidad.

INSTRUCCIONES CR√çTICAS:

1. ANALIZA paso a paso (Chain of Thought):
   - ¬øCu√°l es exactamente la pregunta?
   - ¬øQu√© informaci√≥n relevante hay en el contexto?
   - ¬øCu√°l es la mejor respuesta?

2. RESTRINGE tu respuesta:
   - SOLO usa informaci√≥n del contexto proporcionado
   - Si no tienes la informaci√≥n, responde: "No tengo informaci√≥n sobre esto en los instructivos disponibles."
   - NUNCA inventes especificaciones t√©cnicas

3. ESTRUCTURA tu respuesta:
   - P√°rrafo 1: Respuesta directa a la pregunta
   - P√°rrafo 2: Detalles t√©cnicos (procedimiento, normas, equipos)
   - P√°rrafo 3: Recomendaci√≥n o precauci√≥n (si aplica)

4. ESTILO:
   - Profesional pero amigable
   - M√°ximo 3 p√°rrafos
   - Incluye referencias t√©cnicas (normas ASTM, c√≥digos de instructivo)
   - Espa√±ol formal

Contexto de instructivos disponibles:
{context}

Historial de conversaci√≥n:
{chat_history}

Pregunta del t√©cnico: {question}
"""
```

**Endpoint /chat con Historial:**
```python
@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    query = data.get('message')
    session_id = data.get('session_id', str(uuid.uuid4()))
    
    # 1. Recuperar historial (√∫ltimas 5 interacciones)
    chat_history = get_conversation_history(session_id, limit=5)
    
    # 2. Similarity search
    docs = vectorstore.similarity_search(query, k=5)
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # 3. Construir prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("human", "{question}")
    ])
    
    # 4. Generar respuesta
    chain = prompt | llm
    response = chain.invoke({
        "context": context,
        "chat_history": format_chat_history(chat_history),
        "question": query
    })
    
    # 5. Guardar turno en historial
    save_conversation_turn(session_id, "user", query)
    save_conversation_turn(session_id, "assistant", response.content)
    
    # 6. Log tokens y costo
    log_chat_interaction(session_id, query, response, docs)
    
    return jsonify({
        "response": response.content,
        "session_id": session_id
    })
```

**Endpoints adicionales:**
- `POST /chat` - Enviar mensaje
- `POST /vote` - Votar respuesta (up/down)
- `POST /negative_feedback` - Enviar feedback negativo
- `GET /metrics` - M√©tricas del sistema
- `POST /vectorize` - Re-ingestar documentos

---

### **FASE 3: FRONTEND** üé®

#### **Paso 3.1: Modificar static/index.html**
```html
<title>LabIa - Asistente Virtual de Laboratorio</title>
<h1>üß™ LabIa</h1>
<p>Asistente Virtual del Laboratorio de Control de Calidad</p>

<footer>
    Powered by Google Gemini 2.5 Flash ‚Ä¢ PostgreSQL + pgvector
</footer>
```

#### **Paso 3.2: Modificar static/script.js**
```javascript
const API_BASE_URL = 'http://127.0.0.1:8000';  // Puerto correcto

let sessionId = localStorage.getItem('labia_session_id') || generateUUID();
localStorage.setItem('labia_session_id', sessionId);

async function sendMessage() {
    const response = await fetch(`${API_BASE_URL}/chat`, {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({
            message: userMessage,
            session_id: sessionId
        })
    });
}
```

#### **Paso 3.3: Modificar static/style.css**
```css
:root {
    --primary-color: #2563eb;      /* Azul laboratorio */
    --secondary-color: #10b981;    /* Verde verificaci√≥n */
    --background: #f8fafc;
    --text-color: #1e293b;
}

/* Estilos de identidad laboratorio */
```

---

### **FASE 4: CONTAINERIZACI√ìN** üê≥

#### **Paso 4.1: Actualizar Dockerfile**
```dockerfile
FROM python:3.11.9-slim

# Instalar Java (para tabula-py)
RUN apt-get update && apt-get install -y \
    openjdk-11-jre-headless \
    tesseract-ocr \
    tesseract-ocr-spa \
    && rm -rf /var/lib/apt/lists/*

# Verificar instalaciones
RUN java -version && tesseract --version

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

ENV PYTHONUNBUFFERED=1
ENV PORT=8000

EXPOSE 8000

CMD ["python", "app.py"]
```

#### **Paso 4.2: Configurar docker-compose.yml**
```yaml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: labia_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  app:
    build: .
    environment:
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: labia_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      LLM_MODEL: gemini-2.5-flash-latest
      LLM_TEMPERATURE: 0.3
      LLM_MAX_TOKENS: 2048
    ports:
      - "8000:8000"
    volumes:
      - ./raw:/app/raw:ro
      - ./static:/app/static
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  postgres_data:
```

#### **Paso 4.3: Crear .env.example**
```env
# Google Gemini API
GOOGLE_API_KEY=your_google_api_key_here

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=labia_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password

# LLM Configuration
LLM_MODEL=gemini-2.5-flash-latest
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2048

# Chunking Configuration
CHUNK_SIZE=1024
CHUNK_OVERLAP=150

# Flask
PORT=8000
FLASK_ENV=production
```

---

### **FASE 5: PRUEBAS LOCALES** üß™

#### **Paso 5.1: Ambiente Virtual**
```powershell
# Verificar Python 3.11.9
python --version

# Crear virtualenv
python -m venv venv

# Activar
.\venv\Scripts\Activate.ps1

# Instalar dependencias
pip install -r requirements.txt

# Verificar Java (para tabula-py)
java -version

# Verificar Tesseract (para pytesseract)
tesseract --version
```

#### **Paso 5.2: Configurar .env**
```powershell
# Copiar template
cp .env.example .env

# Editar .env con tu GOOGLE_API_KEY
notepad .env
```

#### **Paso 5.3: Inicializar PostgreSQL Local**
```powershell
# Opci√≥n A: PostgreSQL instalado localmente
# Crear base de datos labia_db
# Instalar extensi√≥n pgvector

# Opci√≥n B: Docker solo para PostgreSQL
docker run -d `
  --name labia_postgres `
  -e POSTGRES_DB=labia_db `
  -e POSTGRES_PASSWORD=yourpassword `
  -p 5432:5432 `
  pgvector/pgvector:pg16
```

#### **Paso 5.4: Ingesta de Prueba (2-3 PDFs)**
```powershell
# Ejecutar ingesta con subset
python ingest.py --test --files "LLCCI02*.pdf,LLCII05*.pdf"

# Verificar:
# - Extracci√≥n de tablas ‚úì
# - Metadatos (c√≥digo, ASTM, revisi√≥n) ‚úì
# - Normalizaci√≥n de unidades ‚úì
# - Chunks en PostgreSQL ‚úì
# - Logs de procesamiento ‚úì
```

#### **Paso 5.5: Pruebas de Queries**
```powershell
# Iniciar app
python app.py

# Probar en browser: http://localhost:8000
```

**Queries de prueba:**
1. "¬øC√≥mo medir el pH seg√∫n la norma ASTM?"
2. "¬øCu√°l es el procedimiento para gravedad espec√≠fica del cemento?"
3. "¬øQu√© temperatura debe tener el agua para el ensayo C109?"
4. "Explica el procedimiento de revenimiento ASTM C143"
5. "¬øQu√© equipos necesito para medir contenido de aire?"

**Validaciones:**
- ‚úì Respuesta directa (p√°rrafo 1)
- ‚úì Detalles t√©cnicos (p√°rrafo 2)
- ‚úì Recomendaci√≥n (p√°rrafo 3)
- ‚úì Referencias a normas ASTM y c√≥digos
- ‚úì Contexto conversacional (5 interacciones)
- ‚úì Fallback "No tengo informaci√≥n" cuando no hay contexto

#### **Paso 5.6: Verificar Logs**
```python
# Consultar PostgreSQL
SELECT session_id, query, tokens_input, tokens_output, cost_usd, latency_ms
FROM chat_logs
ORDER BY timestamp DESC
LIMIT 10;

# Verificar historial conversacional
SELECT session_id, conversation_history
FROM chat_session_state;
```

---

### **FASE 6: MIGRACI√ìN A DOCKER** üöÄ

#### **Paso 6.1: Build y Run**
```powershell
# Build imagen
docker-compose build

# Iniciar servicios
docker-compose up -d

# Verificar logs
docker-compose logs -f app
docker-compose logs -f postgres
```

#### **Paso 6.2: Ingesta Completa (44 PDFs)**
```powershell
# Ejecutar dentro del contenedor
docker-compose exec app python ingest.py

# Monitorear progreso
docker-compose logs -f app
```

**Estimaci√≥n de tiempo:**
- 44 PDFs √ó ~10-15 p√°ginas = ~500 p√°ginas
- ~30-60 segundos por PDF (extracci√≥n + chunking + embeddings)
- **Total:** ~20-40 minutos

#### **Paso 6.3: Validaci√≥n Final**
```powershell
# Acceder a la UI
# http://localhost:8000

# Probar:
# 1. Ingesta completa ‚úì
# 2. Queries complejas ‚úì
# 3. Contexto conversacional ‚úì
# 4. Performance (latencia < 3s) ‚úì
# 5. UI responsive ‚úì
# 6. Feedback/votos funcionando ‚úì
```

---

## üìä M√âTRICAS DE √âXITO

### **Calidad de Respuestas**
- ‚úÖ Precisi√≥n t√©cnica: >90% correcta seg√∫n instructivos
- ‚úÖ Estructura 3 p√°rrafos: 100% cumplimiento
- ‚úÖ Referencias ASTM/c√≥digos: >80% inclusi√≥n
- ‚úÖ Fallback apropiado: 0% alucinaciones

### **Performance**
- ‚úÖ Latencia promedio: <3 segundos
- ‚úÖ Ingesta completa: <45 minutos
- ‚úÖ Precisi√≥n retrieval: >85% documentos relevantes

### **Contexto Conversacional**
- ‚úÖ Mantiene 5 interacciones: 100%
- ‚úÖ Coherencia multi-turn: >90%

### **Costos (Google Gemini 2.5 Flash)**
- üìâ Input: $0.00001875 / 1K tokens
- üìâ Output: $0.000075 / 1K tokens
- üí∞ Estimado mensual (1000 queries): ~$2-5 USD

---

## üîç VALIDACI√ìN POR TIPO DE PDF

### **PDFs Tipo 1: LLCCI (Control de Calidad Interno)**
- ‚úÖ Extracci√≥n de pasos numerados
- ‚úÖ Tablas de resultados (G, T, V, Ph, Pa, Ps)
- ‚úÖ F√≥rmulas matem√°ticas
- ‚úÖ Metadatos: c√≥digo LLCCI-xx, revisi√≥n

### **PDFs Tipo 2: LLCII (Inspecci√≥n Interna)**
- ‚úÖ Normas ASTM (C109, C1090, C143, etc.)
- ‚úÖ Figuras (fracturas, equipos)
- ‚úÖ Unidades mixtas (psi/MPa, mm/in, ¬∞C/¬∞F)
- ‚úÖ Rangos y tolerancias

### **Elementos Especiales**
- ‚úÖ Logos y sellos (ignorados)
- ‚úÖ "DOCUMENTO CONTROLADO" (limpiado)
- ‚úÖ Diacr√≠ticos (¬∞C, ¬±, √ó, √∑)
- ‚úÖ Pies de p√°gina (filtrados)

---

## üö® POSIBLES ISSUES Y SOLUCIONES

### **Issue 1: Java no instalado (tabula-py)**
```powershell
# Windows: Descargar OpenJDK 11
# https://jdk.java.net/11/
# Agregar a PATH

# Docker: Ya incluido en Dockerfile
```

### **Issue 2: Tesseract no instalado (pytesseract)**
```powershell
# Windows: Descargar installer
# https://github.com/UB-Mannheim/tesseract/wiki
# Agregar a PATH

# Docker: Ya incluido en Dockerfile
```

### **Issue 3: PostgreSQL pgvector no disponible**
```sql
-- Conectar a PostgreSQL
CREATE EXTENSION vector;

-- Verificar
SELECT * FROM pg_extension WHERE extname = 'vector';
```

### **Issue 4: Google API Key inv√°lida**
```powershell
# Verificar en Google AI Studio
# https://aistudio.google.com/app/apikey

# Testear
python -c "import google.generativeai as genai; genai.configure(api_key='YOUR_KEY'); print('OK')"
```

### **Issue 5: Chunking rompe tablas**
```python
# Soluci√≥n: Detectar tablas y tratarlas como chunks √∫nicos
if is_table(text):
    chunks.append(text)  # No dividir
else:
    chunks = splitter.split_text(text)
```

---

## üìö RECURSOS Y DOCUMENTACI√ìN

### **APIs y SDKs**
- [Google Gemini API Docs](https://ai.google.dev/docs)
- [LangChain Google Gemini Integration](https://python.langchain.com/docs/integrations/providers/google_generative_ai)
- [pgvector Documentation](https://github.com/pgvector/pgvector)

### **Librer√≠as de PDFs**
- [pdfplumber](https://github.com/jsvine/pdfplumber)
- [PyMuPDF (fitz)](https://pymupdf.readthedocs.io/)
- [pytesseract](https://github.com/madmaze/pytesseract)
- [unstructured](https://unstructured-io.github.io/unstructured/)

### **Normalizaci√≥n**
- [pint - Unit conversions](https://pint.readthedocs.io/)

---

## ‚úÖ CHECKLIST FINAL

### **C√≥digo**
- [ ] requirements.txt creado
- [ ] ingest.py implementado (multi-librer√≠a + normalizaci√≥n)
- [ ] app.py modificado (Google Gemini + historial)
- [ ] database.py modificado (Google embeddings + tablas)
- [ ] static/* adaptado (LabIa branding)

### **Docker**
- [ ] Dockerfile actualizado (Java + Tesseract)
- [ ] docker-compose.yml configurado
- [ ] .env.example creado

### **Pruebas Locales**
- [ ] Ambiente virtual Python 3.11.9
- [ ] Dependencias instaladas
- [ ] PostgreSQL + pgvector funcionando
- [ ] Ingesta 2-3 PDFs exitosa
- [ ] Queries y contexto validados

### **Docker**
- [ ] Build exitoso
- [ ] Servicios arriba (postgres + app)
- [ ] Ingesta completa 44 PDFs
- [ ] UI accesible en http://localhost:8000
- [ ] Performance validada

---

## üéâ PR√ìXIMOS PASOS (POST-VALIDACI√ìN)

1. **Optimizaci√≥n de Retrieval**
   - Implementar re-ranking con cross-encoder
   - Ajustar k din√°micamente (5-15 seg√∫n complejidad)
   - Hybrid search (vector + keyword)

2. **Mejoras de UI**
   - Mostrar documentos fuente citados
   - Exportar conversaci√≥n a PDF
   - Modo offline con cach√©

3. **Analytics**
   - Dashboard de m√©tricas (queries/d√≠a, temas populares)
   - Feedback loop para mejorar chunks
   - A/B testing de prompts

4. **Escalabilidad**
   - Redis para cach√© de embeddings
   - Load balancing
   - Monitoreo con Prometheus + Grafana

---

**Plan creado:** 27 de diciembre de 2025  
**Estimaci√≥n total:** 2-3 d√≠as de desarrollo + pruebas  
**Estado:** üöÄ READY TO IMPLEMENT
